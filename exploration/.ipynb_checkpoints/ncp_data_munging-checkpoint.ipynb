{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data_dir = '/Users/fineiskid/nu/jiping_research/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore `genome_ncp.feather`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_ncp_df = feather.read_dataframe(os.path.join(raw_data_dir, 'genome_ncp.feather'))\n",
    "\n",
    "print('genome_ncp_df.shape {0}'.format(genome_ncp_df.shape))\n",
    "print('unique chromosomes: {0}'.format(len(genome_ncp_df.Chr.unique())))\n",
    "genome_ncp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore `NNT_cutWC.NCP.Ratio.txt`\n",
    "\n",
    "It appears as though `genome_ncp.feather` is just `NNT_cutWC.NCP.Ratio.txt` saved in .feather format,\n",
    "and both files just contain the NCP scores for the entire s. cerevisiae genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnt_df = pd.read_table(os.path.join(raw_data_dir, 'NNT_cutWC.NCP.Ratio.txt')\n",
    "                       , sep = '\\s+')\n",
    "\n",
    "print('nnt_df.shape {0}'.format(genome_ncp_df.shape))\n",
    "print('unique chromosomes: {0}'.format(len(nnt_df.Chr.unique())))\n",
    "nnt_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore `nature11142-s3_corrected_NCP_scores.txt`\n",
    "\n",
    "It appears that nature11142-s3_corrected_NCP_scores.txt contains a subset of rows in NNT_cutWC.NCP.Ratio.txt with high NCP scores. This is probably the redundant map, although \"A map of nucleosome positions in yeast at base-pair resolution\" paper says the redundant map has 351,264 nucleosomes, and this dataset has 344,720 nucleosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncp_df = pd.read_table(os.path.join(raw_data_dir, 'nature11142-s3_corrected_NCP_scores.txt')\n",
    "                       , sep = '\\s+'\n",
    "                       , names = ['Chr', 'pos', 'NCP', 'NCP/noise'])\n",
    "\n",
    "print('ncp_df.shape {0}'.format(ncp_df.shape))\n",
    "print('unique chromosomes: {0}'.format(len(ncp_df.Chr.unique())))\n",
    "print('NCP score range: ({0}, {1})'.format(ncp_df.NCP.min(), ncp_df.NCP.max()))\n",
    "ncp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_ncp_df[(genome_ncp_df.Chr == 'chrI') & (genome_ncp_df.pos.isin([72, 83, 370, 371, 389, 633, 642, 649]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore `seq_ncp_positions_redundant_map.feather`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_df = feather.read_dataframe(os.path.join(raw_data_dir, 'seq_ncp_positions_redundant_map.feather'))\n",
    "\n",
    "print('redundant_df.shape {0}'.format(redundant_df.shape))\n",
    "print('unique chromosomes: {0}'.format(len(redundant_df.Chr.unique())))\n",
    "print('NCP score range: ({0}, {1})'.format(redundant_df['NCP/noise'].min(), redundant_df['NCP/noise'].max()))\n",
    "redundant_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_df.nucleosome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant_nuc_df = unique_df[unique_df.nucleosome == 1.0]\n",
    "\n",
    "print('Nucleosome NCP score range: ({0}, {1})'.format(redundant_nuc_df['NCP/noise'].min(), redundant_nuc_df['NCP/noise'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_df[redundant_df.isnull().any(1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_ncp_df[genome_ncp_df.Chr == 'chrIII'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right...\n",
    "The NCP data doesn't have the full scope of the transcript - random positions are missing. 44,220 positions total. How much of the genome is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0}% of transcript is missing'.format(np.round(100 * (44220 / (12026678 + 44220)), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore `seq_ncp_positions_unique_map.feather`\n",
    "\n",
    "This is the unique map. There are ~67,500 nucleosomes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = feather.read_dataframe(os.path.join(raw_data_dir, 'seq_ncp_positions_unique_map.feather'))\n",
    "\n",
    "print('unique_df.shape {0}'.format(unique_df.shape))\n",
    "print('unique chromosomes: {0}'.format(len(unique_df.Chr.unique())))\n",
    "unique_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.nucleosome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nuc_df = unique_df[unique_df.nucleosome == 1.0]\n",
    "\n",
    "print('Nucleosome NCP score range: ({0}, {1})'.format(unique_nuc_df['NCP/noise'].min(), unique_nuc_df['NCP/noise'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate `seq_ncp_positions_unique_map.feather`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load in the .fa files and stack them, one chromosome at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "fasta_dir = os.path.join(raw_data_dir, 'chromFa')\n",
    "seq_dfs = list()\n",
    "for f in os.listdir(fasta_dir):\n",
    "    if re.search('chr', f) is not None:\n",
    "        seq_dat = SeqIO.read(os.path.join(raw_data_dir, 'chromFa', f), \"fasta\")\n",
    "        seq = str(seq_dat.seq)\n",
    "        chrom = seq_dat.name\n",
    "        \n",
    "        # position is 1-indexed.\n",
    "        seq_dfs.append(pd.DataFrame({'Chr': chrom\n",
    "                                     , 'pos': range(1, len(seq) + 1)\n",
    "                                     , 'seq': [nuc for nuc in seq]}))\n",
    "        \n",
    "seq_df = pd.concat(seq_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('seq_df.shape: {0}'.format(seq_df.shape))\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: join .fa files with `NNT_cutWC.NCP.Ratio.txt` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_nnt_df = seq_df.join(nnt_df.set_index(['Chr', 'pos']), on=['Chr', 'pos'], how='left')\n",
    "seq_nnt_df.reset_index(drop=True, inplace=True)\n",
    "seq_nnt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_nnt_df[seq_nnt_df.Chr == 'chrIII'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: run greedy algorithm.\n",
    "\n",
    "We used a greedy algorithm to make nucleosome calls sequentially based on the magnitude of NCP/noise as follows:\n",
    "- On each chromosome the position that had the largest NCP/noise was first called as the center of a nucleosome.\n",
    "- Then another position with the largest NCP/noise among all positions that were at least +/-107 bp away from the first selected nucleosome was called as a nucleosome center.\n",
    "- This step was repeated such that every selected nucleosome in the current step was at least +/-107 bp away from any previously selected nucleosomes.\n",
    "- The algorithm stopped when no nucleosomes could be further called. By this approach, we allowed a maximum of 40 bp overlap between two neighboring nucleosomes.\n",
    "\n",
    "This could help reduce possible mis-calls due to miscalls\n",
    "from previous rounds in the neighboring regions.\n",
    "Based on the results from four-template model, we selected a total of 75,047 nucleosomes\n",
    "genome-wide, among which, \n",
    "- 67,543 corresponding to 90% that have highest NCP score/noise ratio were further selected in the unique set to represent the collection of most likely nucleosome positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = seq_nnt_df[['Chr', 'pos', 'NCP/noise']].copy()\n",
    "dat = dat[~dat.isnull().any(1)]\n",
    "\n",
    "# sort positions by descending NCP/noise.\n",
    "ncp_sorted_idx = np.argsort(dat['NCP/noise']).values[::-1].tolist()\n",
    "nuc_idx = ncp_sorted_idx[0]\n",
    "\n",
    "nuc_dat = dat.iloc[nuc_idx]\n",
    "nuc_center_df = pd.DataFrame({'Chr': [nuc_dat['Chr']]\n",
    "                              , 'pos': [nuc_dat['pos']]})\n",
    "\n",
    "ctr = 0\n",
    "while True:\n",
    "    \n",
    "    ctr += 1\n",
    "    if ctr % 1000 == 0:\n",
    "        print('Number of nucleosome centers: {0}.'.format(nuc_center_df.shape[0]))\n",
    "    \n",
    "    # Remove selected nuc index from sorted nuc score indices.\n",
    "    del ncp_sorted_idx[0]\n",
    "    \n",
    "    # if no more indices to add, exit.\n",
    "    if len(ncp_sorted_idx) == 0:\n",
    "        break\n",
    "    \n",
    "    nuc_idx = ncp_sorted_idx[0]\n",
    "    nuc_dat = dat.iloc[nuc_idx]\n",
    "    chrom, pos, score = nuc_dat['Chr'], nuc_dat['pos'], nuc_dat['NCP/noise']\n",
    "    \n",
    "    # if adding indices with 0 NCP/noise score, definitely stop.\n",
    "    if score == 0:\n",
    "        break\n",
    "    \n",
    "    # if chrom not yet in nuc_center_df, add nucleosome chrom, pos, because\n",
    "    # distance to existing nuc centers is sufficient.\n",
    "    if chrom not in nuc_center_df.Chr:\n",
    "        nuc_center_df = nuc_center_df.append(pd.DataFrame({'Chr': [chrom]\n",
    "                                                           , 'pos': [pos]}))\n",
    "        continue\n",
    "    \n",
    "    # compute distance of this position to existing nucleosome centers on this chromosome.\n",
    "    chr_nuc_pos_diffs = np.abs(pos - nuc_center_df[nuc_center_df.Chr == chrom].pos.values)\n",
    "    \n",
    "    # if this position less than 107 from an existing nucleosome, don't add.\n",
    "    if np.any(chr_nuc_pos_diffs < 107):\n",
    "        continue\n",
    "    \n",
    "    # otherwise, consider this a new nucleosome.\n",
    "    else:\n",
    "        nuc_center_df = nuc_center_df.append(pd.DataFrame({'Chr': [chrom]\n",
    "                                                           , 'pos': [pos]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_center_join_df = nuc_center_df.join(nnt_df.set_index(['Chr', 'pos']), on=['Chr', 'pos'], how='left')\n",
    "\n",
    "nuc_center_join_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where did `seq_ncp_positions_[unique/redundant]_map.feather` come from??\n",
    "\n",
    "## Show that `seq_ncp_positions_[unique/redundant]_map.feather` is supplement data nucleosome positions, joined with fasta sequences, joined with NCP data `NNT_cutWC.NCP.Ratio.txt`\n",
    "\n",
    "See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3786739/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant nucleosome positions from Nature article.\n",
    "nature_redundant_df = pd.read_table(os.path.join(raw_data_dir, 'NIHMS370046-supplement-4.txt')\n",
    "                                    , sep='\\s+'\n",
    "                                    , names = ['Chr', 'pos', 'NCP', 'NCP/noise'])\n",
    "\n",
    "# unique nucleosome positions from Nature article.\n",
    "nature_unique_df = pd.read_table(os.path.join(raw_data_dir, 'NIHMS370046-supplement-5.txt')\n",
    "                                 , sep='\\s+'\n",
    "                                 , names = ['Chr', 'pos', 'NCP', 'NCP/noise'])\n",
    "\n",
    "# NCP data for whole genome.\n",
    "nnt_df = pd.read_table(os.path.join(raw_data_dir, 'NNT_cutWC.NCP.Ratio.txt')\n",
    "                       , sep = '\\s+')\n",
    "\n",
    "# left join seq data on NCP data (will cause NAs)\n",
    "seq_nnt_df = seq_df.join(nnt_df.set_index(['Chr', 'pos'])\n",
    "                         , on=['Chr', 'pos']\n",
    "                         , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- build full redundant dataset. --------------- #\n",
    "\n",
    "# gather up redundant nucleosome positions\n",
    "redundant_pos_df = nature_redundant_df[['Chr', 'pos']].copy()\n",
    "redundant_pos_df['nucleosome'] = 1.\n",
    "\n",
    "# join seq/ncp data with redundant nuc locations on (chrom, pos) key.\n",
    "redundant_full_df = seq_nnt_df.join(redundant_pos_df.set_index(['Chr', 'pos'])\n",
    "                                    , on=['Chr', 'pos']\n",
    "                                    , how='left')\n",
    "\n",
    "# replace NaN nucleosome indicators (non-nuc center locations from seq_nnt_df)\n",
    "# with zeros.\n",
    "redundant_full_df.loc[redundant_full_df.nucleosome.isnull(), 'nucleosome'] = 0.\n",
    "\n",
    "print('redundant map contains {0} nucleosomes'.format(int(np.sum(redundant_full_df.nucleosome))))\n",
    "\n",
    "# drop locations with missing NCP data.\n",
    "redundant_full_df = redundant_full_df[~redundant_full_df.isnull().any(1)]\n",
    "\n",
    "print('redundant_full_df.shape = {0}'.format(redundant_full_df.shape))\n",
    "print('seq_ncp_positions_redundant_map with eliminated missing NCP data, shape = {0}'\n",
    "      .format(redundant_df[~redundant_df.isnull().any(1)].shape))\n",
    "print('seq_ncp_positions_unique_map with eliminated missing NCP data, shape = {0}'\n",
    "      .format(unique_df[~unique_df.isnull().any(1)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- build full unique dataset. --------------- #\n",
    "\n",
    "# gather up redundant nucleosome positions\n",
    "unique_pos_df = nature_unique_df[['Chr', 'pos']].copy()\n",
    "unique_pos_df['nucleosome'] = 1.\n",
    "\n",
    "# join seq/ncp data with redundant nuc locations on (chrom, pos) key.\n",
    "unique_full_df = seq_nnt_df.join(unique_pos_df.set_index(['Chr', 'pos'])\n",
    "                                 , on=['Chr', 'pos']\n",
    "                                 , how='left')\n",
    "\n",
    "# replace NaN nucleosome indicators (non-nuc center locations from seq_nnt_df)\n",
    "# with zeros.\n",
    "unique_full_df.loc[unique_full_df.nucleosome.isnull(), 'nucleosome'] = 0.\n",
    "\n",
    "print('unique map contains {0} nucleosomes'.format(int(np.sum(unique_full_df.nucleosome))))\n",
    "\n",
    "# drop locations with missing NCP data.\n",
    "unique_full_df = unique_full_df[~unique_full_df.isnull().any(1)]\n",
    "\n",
    "print('unique_full_df.shape = {0}'.format(unique_full_df.shape))\n",
    "print('seq_ncp_positions_redundant_map with eliminated missing NCP data, shape = {0}'\n",
    "      .format(redundant_df[~redundant_df.isnull().any(1)].shape))\n",
    "print('seq_ncp_positions_unique_map with eliminated missing NCP data, shape = {0}'\n",
    "      .format(unique_df[~unique_df.isnull().any(1)].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study continuous run lengths within joined sequence/NCP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "\n",
    "seq_df = pd.read_feather(os.path.join(raw_data_dir, 'unique_nucleosome_map.feather'))\n",
    "seq_df.drop('index'\n",
    "            , axis=1\n",
    "            , inplace=True)\n",
    "\n",
    "shuffle = False  # shuffle between samples in a batch\n",
    "lookback = 250\n",
    "batch_size = 128\n",
    "step_size = lookback\n",
    "\n",
    "chromosomes = seq_df['Chr'].unique().tolist()\n",
    "chromosomes.sort()\n",
    "\n",
    "# identify blocks of nonmissing data, per chromosome so that\n",
    "# consecutive row indices do not bleed over chromosomes.\n",
    "cts_regions = list()\n",
    "\n",
    "# for each chromosome, build up list of [start, end] indices of continuous position runs\n",
    "for chrom in chromosomes:\n",
    "    \n",
    "    # pick out indices of DataFrame for this chromosome.\n",
    "    chrom_idx = np.where(seq_df.Chr == chrom)[0]\n",
    "    chrom_offset = np.min(chrom_idx)\n",
    "    chrom_len = len(chrom_idx)\n",
    "    \n",
    "    sub_df = seq_df.iloc[chrom_idx][['seq', 'pos']]\n",
    "    pos_vec = sub_df.pos.values\n",
    "    \n",
    "    # determine where consecutive change change in pos is > 1\n",
    "    changepoints = np.where(np.diff(pos_vec) > 1)[0]\n",
    "    n_changepoints = len(changepoints)\n",
    "    n_regions = n_changepoints + 1\n",
    "    \n",
    "    # special case for when whole chromosome is one contiguous region.\n",
    "    if n_regions == 1:\n",
    "        cts_regions.append([chrom_offset, chrom_offset + chrom_len])\n",
    "       \n",
    "    # construct [start, end] region row indices.\n",
    "    else:\n",
    "        for i in range(n_regions):\n",
    "            if i == 0:\n",
    "                start = chrom_offset\n",
    "                end = chrom_offset + changepoints[i]\n",
    "            elif i < n_regions - 1:\n",
    "                start = chrom_offset + changepoints[i - 1] + 1\n",
    "                end = chrom_offset + changepoints[i]\n",
    "            else:\n",
    "                start = chrom_offset + changepoints[i - 1] + 1\n",
    "                end = chrom_offset + chrom_len\n",
    "\n",
    "            cts_regions.append([start, end])\n",
    "            \n",
    "    \n",
    "# filter out continuous sub-regions that aren't long enough to build batch specifications.\n",
    "if not shuffle:\n",
    "    scan_regions = [x for x in cts_regions if x[1] - x[0] > lookback*batch_size]\n",
    "    \n",
    "else:\n",
    "    scan_regions = [x for x in cts_regions if x[1] - x[0] > lookback]\n",
    "\n",
    "# determine region lengths.\n",
    "region_lens = [x[1] - x[0] for x in scan_regions]\n",
    "region_lens /= np.sum(region_lens)\n",
    "n_regions = len(scan_regions)\n",
    "\n",
    "# extract raw, unselected training and target data\n",
    "x = seq_df['seq'].values\n",
    "y = seq_df['nucleosome'].values\n",
    "stop = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # storage for indices marking ends of samples.\n",
    "    scan_ends = np.zeros(batch_size)\n",
    "    \n",
    "    # if not shuffling within a batch, randomly select a continuous region to scan,\n",
    "    # and then pick a continuous region of lookback * batch_size scan indices.\n",
    "    if not shuffle:\n",
    "\n",
    "        # randomly select one continuous region.\n",
    "        region = scan_regions[np.random.choice(range(n_regions)\n",
    "                                               , size=1\n",
    "                                               , p=region_lens)[0]]\n",
    "        \n",
    "        # assemble set of possible sample-ends indices.\n",
    "        scan_idx = list(range(region[0] + lookback * batch_size, region[1]))\n",
    "        n_avail_scans = len(scan_idx)\n",
    "        \n",
    "        # randomly select a starting index for batch creation.\n",
    "        i = np.random.choice(range(n_avail_scans)\n",
    "                            , size=1)[0]\n",
    "        \n",
    "        for ii in range(batch_size):\n",
    "\n",
    "            # if index selection is too large, reset to earlier in the cts region.\n",
    "            if i >= n_avail_scans:\n",
    "                i = i % n_avail_scans\n",
    "\n",
    "            # append sample end index from region.\n",
    "            scan_ends[ii] = scan_idx[i]\n",
    "\n",
    "            # within a batch, move to the next subsequence end position.\n",
    "            i += step_size\n",
    "                \n",
    "    # if shuffling within a batch, randomly select continuous regions from which to select\n",
    "    else:\n",
    "        \n",
    "        # randomly select batch_size-many continuous regions, one sample within each selection.\n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            # pick region.\n",
    "            region = scan_regions[np.random.choice(range(n_regions)\n",
    "                                                   , size=1\n",
    "                                                   , p=region_lens)[0]]\n",
    "            \n",
    "            # pick sample end index from region.\n",
    "            scan_ends[ii] = np.random.choice(list(range(region[0] + lookback, region[1]))\n",
    "                                             , size=1)[0]\n",
    "            \n",
    "    # output storage\n",
    "    samples = np.zeros([batch_size, lookback, 4])\n",
    "    \n",
    "    stop += 1\n",
    "    if stop > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_nucleotide_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-27d79fbe77d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_nucleotide_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlookback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0meos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# if testing is specified, identify the indices of the dataframe used in batch's samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embed_nucleotide_seq' is not defined"
     ]
    }
   ],
   "source": [
    "target_position = 'all'\n",
    "testing = True\n",
    "no_one_class_samples = False\n",
    "\n",
    "if target_position == 'all':\n",
    "    sequence_targets = True\n",
    "    target_len = lookback\n",
    "\n",
    "    def target_processor(vec, sub_idx):\n",
    "        return vec[sub_idx], sub_idx\n",
    "\n",
    "# if doing seq2seq learning, targets have to be full sequences (2-d array), otherwise\n",
    "# targets will just be a 1-d array of len(scan_ends), which is typically just batch_size.\n",
    "if sequence_targets:\n",
    "    targets = np.zeros([len(scan_ends), target_len, 1])\n",
    "else:\n",
    "    targets = np.zeros([len(scan_ends), ])\n",
    "\n",
    "if testing:\n",
    "    idx = list()\n",
    "\n",
    "# load up batch's samples and targets.\n",
    "for ii in range(batch_size):\n",
    "    eos = int(scan_ends[ii])\n",
    "\n",
    "    # if positions 0 through 1000 (incl) are permissible, then permissible sample\n",
    "    # end positions are 500 -> 1000. E.g. 500 - 500 = 0, include 0 in sample, run through 499 incl.\n",
    "    targets_tmp, selected_idx = target_processor(y\n",
    "                                                 , sub_idx=np.arange((eos - lookback), eos))\n",
    "\n",
    "    # controlling if targets are sequences different than if they're scalars.\n",
    "    if sequence_targets:\n",
    "\n",
    "        # if user does not want samples where target sequences are all one value (e.g. negative class),\n",
    "        # try 1000 times to resample positions until a target sequence with > 1 classes found.\n",
    "        if no_one_class_samples:\n",
    "            stddev = np.std(targets_tmp)\n",
    "            max_tries = 1000\n",
    "            try_ctr = 1\n",
    "            while stddev == 0.0 and try_ctr <= max_tries:\n",
    "                if shuffle:\n",
    "                    eos = int(np.random.choice(scan_idx\n",
    "                                               , size=1))\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        'Sequential batch creation zero-variance avoidance not implemented.')\n",
    "\n",
    "                targets_tmp, selected_idx = target_processor(y\n",
    "                                                             , sub_idx=np.arange((eos - lookback), eos))\n",
    "                try_ctr += 1\n",
    "                stddev = np.std(targets_tmp)\n",
    "\n",
    "        targets[ii] = np.expand_dims(targets_tmp, axis=1)\n",
    "    else:\n",
    "        targets[ii] = targets_tmp\n",
    "\n",
    "    samples[ii] = embed_nucleotide_seq(x[(eos - lookback) : eos])\n",
    "\n",
    "    # if testing is specified, identify the indices of the dataframe used in batch's samples.\n",
    "    if testing:\n",
    "        idx.extend(selected_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11982754.,  11983004.,  11983254.,  11983504.,  11983754.,\n",
       "        11984004.,  11984254.,  11984504.,  11984754.,  11985004.,\n",
       "        11985254.,  11985504.,  11985754.,  11986004.,  11986254.,\n",
       "        11986504.,  11986754.,  11987004.,  11987254.,  11987504.,\n",
       "        11987754.,  11988004.,  11988254.,  11988504.,  11988754.,\n",
       "        11989004.,  11989254.,  11989504.,  11989754.,  11990004.,\n",
       "        11990254.,  11990504.,  11990754.,  11991004.,  11991254.,\n",
       "        11991504.,  11991754.,  11992004.,  11992254.,  11992504.,\n",
       "        11992754.,  11993004.,  11993254.,  11993504.,  11993754.,\n",
       "        11994004.,  11994254.,  11994504.,  11994754.,  11995004.,\n",
       "        11995254.,  11995504.,  11995754.,  11996004.,  11996254.,\n",
       "        11996504.,  11996754.,  11997004.,  11997254.,  11997504.,\n",
       "        11997754.,  11998004.,  11998254.,  11998504.,  11998754.,\n",
       "        11999004.,  11999254.,  11999504.,  11999754.,  12000004.,\n",
       "        12000254.,  12000504.,  12000754.,  12001004.,  12001254.,\n",
       "        12001504.,  12001754.,  12002004.,  12002254.,  12002504.,\n",
       "        12002754.,  12003004.,  12003254.,  12003504.,  12003754.,\n",
       "        12004004.,  12004254.,  12004504.,  12004754.,  12005004.,\n",
       "        12005254.,  12005504.,  12005754.,  12006004.,  12006254.,\n",
       "        12006504.,  12006754.,  12007004.,  12007254.,  12007504.,\n",
       "        12007754.,  12008004.,  12008254.,  12008504.,  12008754.,\n",
       "        12009004.,  12009254.,  12009504.,  12009754.,  12010004.,\n",
       "        12010254.,  12010504.,  12010754.,  12011004.,  12011254.,\n",
       "        12011504.,  12011754.,  12012004.,  12012254.,  12012504.,\n",
       "        12012754.,  12013004.,  12013254.,  12013504.,  12013754.,\n",
       "        12014004.,  12014254.,  12014504.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
